{"cells":[{"cell_type":"markdown","metadata":{},"source":["##### Importing Packages"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-02-18T06:27:54.997903Z","iopub.status.busy":"2024-02-18T06:27:54.997560Z","iopub.status.idle":"2024-02-18T06:28:05.578318Z","shell.execute_reply":"2024-02-18T06:28:05.577535Z","shell.execute_reply.started":"2024-02-18T06:27:54.997877Z"},"trusted":true},"outputs":[],"source":["import torch\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","from sentence_transformers import SentenceTransformer\n","from rouge import Rouge\n","from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n","from sentence_transformers import SentenceTransformer, util\n","from tqdm.auto import tqdm\n","import numpy as np\n","import random\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Selecting cpu or gpu based on device\n","\n","device = \"cpu\"\n","\n","if torch.backends.mps.is_available():\n","    device = \"mps\"\n","elif torch.cuda.is_available():\n","    device = \"cuda\"\n","else:\n","    device = \"cpu\"\n","\n","print(f\"Using {device} device.\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Parameters\n","\n","MAX_LENGTH = 1024  \n","\n","TEMPRATURE = 0.7  \n","\n","REPETITION_PENALTY = 1.0\n","\n","TOP_K = 50  \n","\n","TOP_P = 0.92 \n","\n","DO_SAMPLE = True  \n","\n","NUM_RETUTNR_SEQUENCES = 1 \n","\n","TXT_DATA = 'Data/Final_merged_data.txt'\n","\n","MODEL_PATH = \"Models/New_E6\""]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/plain":["1"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# Load pre-trained model and tokenizer\n","model = GPT2LMHeadModel.from_pretrained(MODEL_PATH).to('cuda')\n","\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","tokenizer.add_special_tokens({'pad_token': '[PAD]'})"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of parameters in the model (in millions): 254.814208\n"]}],"source":["# Number of parameters in Millions\n","\n","num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","# Convert number of parameters to millions\n","num_params_millions = num_params / 1_000_000\n","\n","print(\"Number of parameters in the model (in millions):\", num_params_millions)\n"]},{"cell_type":"markdown","metadata":{},"source":["#### Evaluation"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# Seperating conversation for evaluation\n","\n","def parse_demo_txt(file_path, samples):\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        lines = file.readlines()\n","\n","    # Take 100 random lines from the file\n","    lines_sampled = random.sample(lines, samples)\n","    all_lines = []\n","    for line in lines_sampled:\n","        segments = line.split('gpt:')\n","        parsed_data = []\n","        for segment in segments[:-1]:  # Exclude the last segment as it will not have a corresponding gpt part\n","            human_part = segment.split('human:')[-1].strip()\n","            gpt_part = segments[segments.index(segment) + 1].split('human:')[0].strip()\n","            parsed_data.append((human_part, gpt_part))\n","        all_lines.append(parsed_data)\n","    \n","    return all_lines\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["\n","def generate_response(model, tokenizer, conversation_history, new_message, max_length=1024):\n","\n","    if len(conversation_history) > 0:\n","        input_text = conversation_history + ' human: ' + new_message + \" gpt:\"\n","    else:\n","        input_text = 'human: ' + new_message + ' gpt:'\n","\n","    input_ids = tokenizer.encode(input_text, return_tensors='pt').to('cuda')\n","\n","    output_sequences = model.generate(\n","        input_ids=input_ids,\n","        max_length=max_length,\n","        # num_beams=5,\n","        temperature=TEMPRATURE,\n","        repetition_penalty=REPETITION_PENALTY,\n","        top_k=TOP_K,\n","        top_p=TOP_P,\n","        do_sample=DO_SAMPLE,\n","        num_return_sequences=NUM_RETUTNR_SEQUENCES,\n","        pad_token_id=tokenizer.pad_token_id,  # Set pad token ID to EOS token ID\n","        attention_mask=input_ids.new_ones(input_ids.shape),  # Provide attention mask\n","        early_stopping=True, # Stop generation when a pad token is generated,\n","    )\n","        \n","    ### Decode and print generated text sequences\n","    for output_sequence in output_sequences:\n","        output_sequence = [token.item() for token in output_sequence if token is not None]\n","\n","        first_index = output_sequence.index(50257)\n","        output_sequence = output_sequence[:first_index]\n","        response = tokenizer.decode(output_sequence)\n","\n","    last_index = input_text.rfind(\"gpt:\")\n","    response = response[last_index+5:]\n","\n","    last_index = response.find(\" human\")\n","    response = response[:last_index]\n","    updated_history = input_text + ' ' + response\n","    \n","    return response, updated_history  # Keep only the most recent part fitting the max_length"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# Initialize models and tools\n","rouge = Rouge()\n","sentence_model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n","\n","rouge_scores = []\n","bleu_scores = []\n","distilbert_scores = []\n","\n","def evaluation(txt_path, num_samples, model, tokenizer):\n","\n","    parsed_data = parse_demo_txt(txt_path, num_samples)\n","\n","    for data in parsed_data:\n","        \n","        conversation_history = \"\"\n","\n","        for human_part, gpt_part in data:\n","            # Simulate GPT generation as reusing gpt_part\n","            generated_gpt_response, conversation_history = generate_response(model, tokenizer, conversation_history, human_part, max_length=1024)\n","            reference_gpt_response = gpt_part  \n","\n","            # ROUGE\n","            scores = rouge.get_scores(generated_gpt_response, reference_gpt_response, avg=True)\n","            rouge_scores.append(scores['rouge-1']['f'])\n","            \n","            # BLEU\n","            reference_tokens = [reference_gpt_response.split()]\n","            generated_tokens = generated_gpt_response.split()\n","            bleu_score = sentence_bleu(reference_tokens, generated_tokens, smoothing_function=SmoothingFunction().method1)\n","            bleu_scores.append(bleu_score)\n","            \n","            # DistilBERT\n","            gen_embed = sentence_model.encode([generated_gpt_response])\n","            ref_embed = sentence_model.encode([reference_gpt_response])\n","            cosine_sim = util.pytorch_cos_sim(gen_embed, ref_embed)\n","            distilbert_scores.append(cosine_sim.item())\n","\n","    # Calculate averages\n","    avg_rouge_score = np.mean(rouge_scores)\n","    avg_bleu_score = np.mean(bleu_scores)\n","    avg_distilbert_score = np.mean(distilbert_scores)\n","\n","    print(f\"Average ROUGE-1 F1: {avg_rouge_score}\")\n","    print(f\"\\nAverage BLEU: {avg_bleu_score}\")\n","    print(f\"\\nAverage DistilBERT Cosine Similarity: {avg_distilbert_score}\")\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\dmaka\\anaconda3\\envs\\Pytorch2.0\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:453: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["Average ROUGE-1 F1: 0.3692968740147636\n","Average BLEU: 0.09851205911397291\n","Average DistilBERT Cosine Similarity: 0.805561407919853\n"]}],"source":["model = model.to('cuda')\n","\n","evaluation(txt_path=TXT_DATA, num_samples=10, model=model, tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4456699,"sourceId":7645783,"sourceType":"datasetVersion"},{"datasetId":4456704,"sourceId":7645795,"sourceType":"datasetVersion"},{"datasetId":4457132,"sourceId":7646530,"sourceType":"datasetVersion"}],"dockerImageVersionId":30646,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
