{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device.\n"
     ]
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Using {device} device.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "\n",
    "MAX_LENGTH = 1024  \n",
    "\n",
    "TEMPRATURE = 0.7  \n",
    "\n",
    "REPETITION_PENALTY = 1.0\n",
    "\n",
    "TOP_K = 50  \n",
    "\n",
    "TOP_P = 0.92 \n",
    "\n",
    "DO_SAMPLE = True  \n",
    "\n",
    "NUM_RETUTNR_SEQUENCES = 1 \n",
    "\n",
    "MODEL_PATH = \"Models/New_E6/checkpoint-42000/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(MODEL_PATH).to('cuda')\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_response(model, tokenizer, conversation_history, new_message, max_length=1024):\n",
    "\n",
    "    if len(conversation_history) > 0:\n",
    "        input_text = conversation_history + ' human: ' + new_message + \" gpt:\"\n",
    "    else:\n",
    "        input_text = 'human: ' + new_message + ' gpt:'\n",
    "\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to('cuda')\n",
    "\n",
    "    output_sequences = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        max_length=max_length,\n",
    "        temperature=TEMPRATURE,\n",
    "        repetition_penalty=REPETITION_PENALTY,\n",
    "        top_k=TOP_K,\n",
    "        top_p=TOP_P,\n",
    "        do_sample=DO_SAMPLE,\n",
    "        num_return_sequences=NUM_RETUTNR_SEQUENCES,\n",
    "        pad_token_id=tokenizer.pad_token_id,  # Set pad token ID to EOS token ID\n",
    "        attention_mask=input_ids.new_ones(input_ids.shape),  # Provide attention mask\n",
    "        early_stopping=True, # Stop generation when a pad token is generated,\n",
    "    )\n",
    "        \n",
    "    ### Decode and print generated text sequences\n",
    "    for output_sequence in output_sequences:\n",
    "        output_sequence = [token.item() for token in output_sequence if token is not None]\n",
    "\n",
    "        first_index = output_sequence.index(50257)\n",
    "        output_sequence = output_sequence[:first_index]\n",
    "        response = tokenizer.decode(output_sequence)\n",
    "\n",
    "    last_index = input_text.rfind(\"gpt:\")\n",
    "    response = response[last_index+5:]\n",
    "\n",
    "    last_index = response.find(\" human\")\n",
    "    response = response[:last_index]\n",
    "    updated_history = input_text + ' ' + response\n",
    "    \n",
    "    return response, updated_history  # Keep only the most recent part fitting the max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Input :  hi.\n",
      "\n",
      "\n",
      "response: hello there. tell me how are you feeling today?\n",
      "\n",
      "\n",
      "Input :  please help me.\n",
      "\n",
      "\n",
      "response: hello, please tell me your problem so that i can help you.\n",
      "\n",
      "\n",
      "Input :  my parents are not listening to me.\n",
      "\n",
      "\n",
      "response: can you tell me the specific situation?\n",
      "\n",
      "\n",
      "Input :  they always make me do what they want and don't care about my feelings.\n",
      "\n",
      "\n",
      "response: this is a very common situation. you can try to communicate with them and tell them your thoughts and feelings.\n",
      "\n",
      "\n",
      "Input :  how can i do that?\n",
      "\n",
      "\n",
      "response: you can try to express your thoughts and feelings to them in a calm tone.\n"
     ]
    }
   ],
   "source": [
    "ans = str(input(\"Want to start the conversation? (y/n) : \")).lower()\n",
    "conversation_history = \"\"\n",
    "\n",
    "while ans == 'y':\n",
    "    new_message = str(input(\"Enter the prompt : \")).lower()\n",
    "    print('\\n\\nInput : ', new_message)\n",
    "    response, conversation_history = generate_response(model, tokenizer, conversation_history, new_message, max_length=1024)\n",
    "    print('\\n\\nresponse:', response)\n",
    "    ans = str(input(\"Want to continue the conversation? (y/n) : \"))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
